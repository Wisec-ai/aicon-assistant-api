# Arquitectura del Sistema

## üìê Visi√≥n General

El **AICON Assistant API** es un microservicio de conversaci√≥n inteligente basado en RAG (Retrieval Augmented Generation) que forma parte del ecosistema **ImmobAI**. Este servicio est√° dise√±ado para procesar consultas de usuarios utilizando documentaci√≥n espec√≠fica de la inmobiliaria, proporcionando respuestas contextuales y precisas mediante streaming.

## üèóÔ∏è Arquitectura de Microservicios

### Contexto dentro de ImmobAI

Este servicio es parte de una arquitectura de microservicios que incluye:

- **Frontend Angular**: Interface web para usuarios e inmobiliarias
- **Backend Principal**: API de gesti√≥n inmobiliaria (propiedades, leads, asesores)
- **AICON Assistant API** (Este servicio): Motor de chat conversacional con RAG
- **WhatsApp Assistant**: Integraci√≥n externa para atenci√≥n de clientes
- **Servicios de Almacenamiento**: Google Cloud Storage y PostgreSQL

### Responsabilidades del Servicio

- ‚úÖ B√∫squeda sem√°ntica en documentos almacenados
- ‚úÖ Generaci√≥n de respuestas contextuales usando Gemini
- ‚úÖ Streaming de respuestas en tiempo real
- ‚úÖ Gesti√≥n de sesiones de conversaci√≥n
- ‚úÖ Persistencia de historial de chat
- ‚úÖ Integraci√≥n con Vertex AI Search (Discovery Engine)

## üîß Stack Tecnol√≥gico

### Backend
- **Framework**: Django 5.1.6 con Django REST Framework
- **Lenguaje**: Python 3.10
- **Base de Datos**: SQLite (desarrollo) / PostgreSQL (producci√≥n)

### IA y Machine Learning
- **LLM**: Google Gemini 1.5 Pro-002
- **RAG Framework**: Langchain Google Community
- **B√∫squeda**: Vertex AI Search (Discovery Engine)
- **Streaming**: Vertex AI Generative Models

### Infraestructura Cloud
- **Plataforma**: Google Cloud Platform
- **Contenedorizaci√≥n**: Docker
- **Despliegue**: Google Cloud Run (Knative)
- **CI/CD**: Google Cloud Build
- **Storage**: Google Cloud Storage
- **Sistema de Archivos**: GCS FUSE

### Librer√≠as Principales
```python
langchain-google-community==1.0.7  # Conexi√≥n con Vertex AI Search
langchain-google-genai==1.0.8      # Gemini Models
langchain-google-vertexai==1.0.8   # Vertex AI Integration
google-cloud-discoveryengine==0.12.0  # Document Search
google-cloud-storage==2.18.0       # Cloud Storage
google-cloud-pubsub==2.15.0        # Pub/Sub messaging
```

## üì¶ Estructura del Proyecto

```
aicon-assistant-api/
‚îú‚îÄ‚îÄ agent/                              # M√≥dulo principal de conversaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ application/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ service/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ response_question.py    # L√≥gica de generaci√≥n de respuestas
‚îÇ   ‚îú‚îÄ‚îÄ domain/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ constants/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ domain_constants.py     # Constantes del dominio
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompts.py              # Templates de prompts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entities/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatDocumentoInfoRequest.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ GenerateSessionRequest.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ repository/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ ai_retriver.py          # Cliente RAG con Vertex AI Search
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ transformer_si.py       # Transformaci√≥n de system instructions
‚îÇ   ‚îú‚îÄ‚îÄ views/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent_conversation.py       # API de chat streaming
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ session.py                  # API de gesti√≥n de sesiones
‚îÇ   ‚îú‚îÄ‚îÄ models.py                       # Models de BD (Session, Conversation)
‚îÇ   ‚îî‚îÄ‚îÄ urls.py                         # Rutas del m√≥dulo agent
‚îÇ
‚îú‚îÄ‚îÄ commons/                            # Utilidades compartidas
‚îÇ   ‚îî‚îÄ‚îÄ domain/
‚îÇ       ‚îú‚îÄ‚îÄ constants/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ env_variables.py        # Variables de entorno
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ domain_constants.py     # Constantes globales
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ exceptions.py           # Excepciones personalizadas
‚îÇ       ‚îî‚îÄ‚îÄ repository/
‚îÇ           ‚îú‚îÄ‚îÄ generative_model.py     # Wrapper de Gemini LLM
‚îÇ           ‚îú‚îÄ‚îÄ data_store.py           # Operaciones de almacenamiento
‚îÇ           ‚îî‚îÄ‚îÄ gcloud_storage_client.py # Cliente GCS
‚îÇ
‚îú‚îÄ‚îÄ documents/                          # Gesti√≥n de documentos
‚îÇ   ‚îú‚îÄ‚îÄ application/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ service.py                  # Servicios de documentos
‚îÇ   ‚îî‚îÄ‚îÄ domain/
‚îÇ       ‚îî‚îÄ‚îÄ models/
‚îÇ           ‚îú‚îÄ‚îÄ document_info.py
‚îÇ           ‚îî‚îÄ‚îÄ data_store_info.py
‚îÇ
‚îú‚îÄ‚îÄ backend_aicon_assistant/            # Configuraci√≥n Django
‚îÇ   ‚îú‚îÄ‚îÄ settings.py                     # Configuraci√≥n principal
‚îÇ   ‚îî‚îÄ‚îÄ urls.py                         # URLs ra√≠z
‚îÇ
‚îú‚îÄ‚îÄ Dockerfile                          # Imagen Docker
‚îú‚îÄ‚îÄ requirements.txt                    # Dependencias Python
‚îú‚îÄ‚îÄ service.yaml                        # Config Knative Cloud Run
‚îî‚îÄ‚îÄ before-cloudbuild.yaml             # Pipeline CI/CD
```

## üîÑ Flujo de Datos

### 1. Generaci√≥n de Sesi√≥n

```
Cliente ‚Üí POST /aicon/chat/generate-session
         ‚Üì
    SessionAPI.create()
         ‚Üì
    Session.objects.create(email, uuid)
         ‚Üì
    Response: { session_id: "uuid..." }
```

### 2. Chat con Streaming

```
Cliente ‚Üí POST /aicon/chat/streaming-chat
         ‚Üì
    AgentConversationAPI.generate_stream_response()
         ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ 1. Recuperaci√≥n RAG                ‚îÇ
    ‚îÇ    AiRetriver.get_few_examples()    ‚îÇ
    ‚îÇ    ‚Üí Vertex AI Search               ‚îÇ
    ‚îÇ    ‚Üí Documentos relevantes          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ 2. Construcci√≥n de Prompt          ‚îÇ
    ‚îÇ    TransformerSystemInstruction()    ‚îÇ
    ‚îÇ    ‚Üí DEFAULT_PROMPT + few_examples  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ 3. Generaci√≥n con LLM              ‚îÇ
    ‚îÇ    ResponseQuestion.generate()       ‚îÇ
    ‚îÇ    ‚Üí Gemini 1.5 Pro                 ‚îÇ
    ‚îÇ    ‚Üí Streaming response             ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ 4. Persistencia                    ‚îÇ
    ‚îÇ    ConversationInfo.objects.create() ‚îÇ
    ‚îÇ    ConversationDetails.objects.create()‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì
    StreamingHttpResponse (SSE)
```

## üéØ Componentes Principales

### AiRetriver
**Responsabilidad**: B√∫squeda sem√°ntica en Vertex AI Search

```python
class AiRetriver:
    def __init__(self, max_documents: int)
    def invoke(query: str) -> List[Document]
    def get_few_examples(query: str) -> str
```

**Caracter√≠sticas**:
- Utiliza `VertexAISearchRetriever` de Langchain
- Busca documentos relevantes basados en similitud sem√°ntica
- Extrae ejemplos contextually relevant para few-shot learning
- Configurable `max_documents` para controlar contexto

### LlmGenerativeModel
**Responsabilidad**: Wrapper de Gemini LLM

```python
class LlmGenerativeModel:
    def __init__(system_instruction: str, type_model: str)
    def generate_content(query, config, safety_settings, stream)
    def get_text_from_iterator(iterator_response)
```

**Configuraci√≥n**:
- Modelo: `gemini-1.5-pro-002` (robust) o `gemini-1.5-flash-002` (flash)
- Temperature: 1.0 (creatividad)
- Max tokens: 8192
- Safety settings: OFF (configurables)

### ResponseQuestion
**Responsabilidad**: Orquestaci√≥n de generaci√≥n de respuestas

```python
class ResponseQuestion:
    def generate_async_response_by_question(
        system_instruction: str,
        question: str
    ) -> Iterator[TextChunk]
```

**Caracter√≠sticas**:
- Integra retriever RAG con generaci√≥n LLM
- Streaming en tiempo real
- Manejo de errores robusto

### Models de Base de Datos

#### Session
- Almacena sesiones por usuario (email)
- UUID √∫nico para tracking
- Timestamps de creaci√≥n y actualizaci√≥n

#### ConversationInfo
- Agrupa conversaciones en una sesi√≥n
- T√≠tulo de conversaci√≥n
- Relaci√≥n ForeignKey con Session

#### ConversationDetails
- Preguntas y respuestas individuales
- Contenido completo del LLM response
- Relaci√≥n ForeignKey con ConversationInfo

## üîê Seguridad y Configuraci√≥n

### Variables de Entorno Requeridas

```bash
PROJECT_ID=your-gcp-project-id
REGION=us-east4
DATA_STORE_ID=your-discovery-engine-datastore
DATA_STORE_LOCATION=global
GCLOUD_STORAGE_BUCKET=your-bucket-name
GCLOUD_STORAGE_FOLDER=your-folder-path
DOCUMENT_BUCKET=your-documents-bucket
TOPIC_DOCUMENT_PROCESSOR=your-pubsub-topic
```

### Autenticaci√≥n

- **Desarrollo**: Sin autenticaci√≥n (configurado para testing)
- **Producci√≥n**: Requiere Service Account de GCP
- **Headers**: Configurables via Django settings

### Rate Limiting

- Implementado via `AnonRateThrottle` de DRF
- Configurable por ruta

## üöÄ Deployment

### Cloud Run Configuration

```yaml
spec:
  containerConcurrency: 80
  timeoutSeconds: 30
  resources:
    limits:
      cpu: '1'
      memory: 1Gi
  containers:
    - image: us-east4-docker.pkg.dev/...
      env:
        - PROJECT_ID
        - REGION
        - DATA_STORE_ID
        # ... m√°s variables
```

### Auto-Scaling

- Min Scale: 1 instancia
- Max Scale: 10 instancias
- Basado en concurrencia y latencia

## üìä Consideraciones de Performance

- **Streaming**: Respuestas en tiempo real, no espera completitud
- **Retrieval**: B√∫squeda optimizada con √≠ndices sem√°nticos de Vertex AI
- **Caching**: Posible implementaci√≥n futura de cach√© de documentos
- **Latency**: T√≠picamente 2-5 segundos para primera respuesta
- **Throughput**: 80 requests concurrentes por instancia

## üîÆ Mejoras Futuras

- [ ] Implementar cach√© Redis para documentos frecuentes
- [ ] A√±adir m√©tricas de observabilidad (Prometheus/Cloud Monitoring)
- [ ] Soporte multi-idioma
- [ ] Fine-tuning de prompts por instituci√≥n
- [ ] Integraci√≥n con m√°s LLMs (configurable)
- [ ] WebSocket support para bidirectional streaming
- [ ] Rate limiting por usuario/sesi√≥n
- [ ] Implementar circuit breakers para resiliencia

## üìö Referencias

- [Vertex AI Search Documentation](https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction)
- [Django REST Framework](https://www.django-rest-framework.org/)
- [Langchain Documentation](https://python.langchain.com/)
- [Google Cloud Run](https://cloud.google.com/run/docs)

